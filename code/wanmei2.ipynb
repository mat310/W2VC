{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training word2vec modell\n"
     ]
    }
   ],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "#from gensim.models.word2vec import LineSentence\n",
    "import gensim\n",
    "import gzip\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "word2vec_modell = 'wanmei'\n",
    "Embsize = 150\n",
    "stride = 1\n",
    "Embepochs = 50\n",
    "kmer_len3 = 3\n",
    "kmer_len4 = 4\n",
    "kmer_len5 = 5\n",
    "kmer_len6 = 6\n",
    "\n",
    "def Gen_Words(sequences,kmer_len,s):\n",
    "\t\tout=[]\n",
    "\n",
    "\t\tfor i in sequences:\n",
    "\n",
    "\t\t\t\tkmer_list=[]\n",
    "\t\t\t\tfor j in range(0,(len(i)-kmer_len)+1,s):\n",
    "\n",
    "\t\t\t\t\t\t\tkmer_list.append(i[j:j+kmer_len])\n",
    "\n",
    "\t\t\t\tout.append(kmer_list)\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "def train(sequences,kmer_len):\n",
    "\tprint('training word2vec modell')\n",
    "\tdocument= Gen_Words(sequences,kmer_len,stride)\n",
    "\t#print(document)\n",
    "\tmodell = gensim.models.Word2Vec (document, window=int(12 / stride), min_count=0, size=Embsize,workers=multiprocessing.cpu_count())\n",
    "\tmodell.train(document,total_examples=len(document),epochs=Embepochs)\n",
    "\tmodell.save(word2vec_modell+str(kmer_len))\n",
    "\treturn document\n",
    "\n",
    "def read_fasta_file():\n",
    "    '''\n",
    "    used for load fasta data and transformd into numpy.array format\n",
    "    '''\n",
    "    fh = open('wanmei.txt', 'r')\n",
    "    seq = []\n",
    "    for line in fh:\n",
    "        if line.startswith('>'):\n",
    "            continue\n",
    "        else:\n",
    "            seq.append(line.replace('\\n', '').replace('\\r', ''))\n",
    "    fh.close()\n",
    "    matrix_data = np.array([list(e) for e in seq])\n",
    "    #print(matrix_data)\n",
    "    return seq\n",
    "\n",
    "sequences = read_fasta_file()\n",
    "document3=train(sequences,kmer_len3)\n",
    "document4=train(sequences,kmer_len4)\n",
    "document5=train(sequences,kmer_len5)\n",
    "document6=train(sequences,kmer_len6)\n",
    "\n",
    "model3 = gensim.models.Word2Vec.load(word2vec_modell+str(kmer_len3))\n",
    "model4 = gensim.models.Word2Vec.load(word2vec_modell+str(kmer_len4))\n",
    "model5 = gensim.models.Word2Vec.load(word2vec_modell+str(kmer_len5))\n",
    "model6 = gensim.models.Word2Vec.load(word2vec_modell+str(kmer_len6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#softmax: 在多分类中常用的激活函数，是基于逻辑回归的。\n",
    "#softplus：softplus(x)=log(1+e^x)，近似生物神经激活函数，最近出现的。\n",
    "#Relu：近似生物神经激活函数，最近出现的。\n",
    "#tanh：双曲正切激活函数，也是很常用的。\n",
    "#sigmoid：S型曲线激活函数，最常用的。\n",
    "#hard_sigmoid：基于S型激活函数。\n",
    "#linear：线性激活函数，最简单的\n",
    "\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        pdf = PdfPages('x.pdf')\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        plt.rcParams['figure.dpi'] = 350\n",
    "\n",
    "     # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"center right\")\n",
    "        plt.title('Acc-loss curve of AD_RFHCP')\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "        pdf.close()\n",
    "        plt.show()\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "for i in range(0,17808):\n",
    "    s=[]\n",
    "#     for word in document3[i]:\n",
    "#         s.append(model3.wv[word])\n",
    "    \n",
    "    for word in document3[i]:\n",
    "        s.append(model3.wv[word])\n",
    "\n",
    "#     for word in document5[i]:\n",
    "#         s.append(model5.wv[word])\n",
    "        \n",
    "        \n",
    "        \n",
    "    X_train.append(s)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# print(np.shape(X_train))\n",
    "\n",
    "# for word2 in document4:\n",
    "#     s=[]\n",
    "#     for i2 in word2:\n",
    "#         s.append(model4.wv[i2])\n",
    "#     X_train.append(s)\n",
    "# print(np.shape(X_train))\n",
    "\n",
    "# for word3 in document5:\n",
    "#     s=[]\n",
    "#     for i3 in word3:\n",
    "#         s.append(model5.wv[i3])\n",
    "#     X_train.append(s)\n",
    "# print(np.shape(X_train))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17808, 39, 150)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as around\n",
    "X_train=np.around(X_train,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#t_x=pd.read_csv('wanmeir.csv', header=None, index_col=None)\n",
    "t_y=pd.read_csv('w_Y.csv', header=None, index_col=None)\n",
    "\n",
    "xx= np.array(X_train)\n",
    "xx_y=t_y.values\n",
    "\n",
    "\n",
    "#xx = np.expand_dims(xx, axis=2)\n",
    "\n",
    "\n",
    "\n",
    "#tt_x=pd.read_csv('fufufud.csv', header=None, index_col=None)\n",
    "#tt_y=pd.read_csv('fuy.csv', header=None, index_col=None)\n",
    "\n",
    "#test_x=tt_x.values\n",
    "#test_y=tt_y.values\n",
    "\n",
    "\n",
    "#test_x = np.expand_dims(test_x, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import math\n",
    "def performance(labelArr, predictArr):\n",
    "    #labelArr[i] is actual value,predictArr[i] is predict value\n",
    "    TP = 0.; TN = 0.; FP = 0.; FN = 0.\n",
    "    for i in range(len(labelArr)):\n",
    "        if labelArr[i] == 1 and predictArr[i] == 1:\n",
    "            TP += 1.\n",
    "        if labelArr[i] == 1 and predictArr[i] == 0:\n",
    "            FN += 1.\n",
    "        if labelArr[i] == 0 and predictArr[i] == 1:\n",
    "            FP += 1.\n",
    "        if labelArr[i] == 0 and predictArr[i] == 0:\n",
    "            TN += 1.\n",
    "    if (TP + FN)==0:\n",
    "        SN=0\n",
    "    else:\n",
    "        SN = TP/(TP + FN) #Sensitivity = TP/P  and P = TP + FN\n",
    "    if (FP+TN)==0:\n",
    "        SP=0\n",
    "    else:\n",
    "        SP = TN/(FP + TN) #Specificity = TN/N  and N = TN + FP\n",
    "    if (TP+FP)==0:\n",
    "        precision=0\n",
    "    else:\n",
    "        precision=TP/(TP+FP)\n",
    "    if (TP+FN)==0:\n",
    "        recall=0\n",
    "    else:\n",
    "        recall=TP/(TP+FN)\n",
    "    GM=math.sqrt(recall*SP)\n",
    "    #MCC = (TP*TN-FP*FN)/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "    return precision,recall,SN,SP,GM,TP,TN,FP,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11396, 39, 150) (3562, 39, 150) (2850, 39, 150)\n",
      "<keras.callbacks.callbacks.ReduceLROnPlateau object at 0x000001B2AE060D08>\n",
      "Starting training \n",
      "Train on 11396 samples, validate on 2850 samples\n",
      "Epoch 1/15\n",
      "11396/11396 [==============================] - 9s 766us/step - loss: 0.4647 - accuracy: 0.8446 - val_loss: 0.3102 - val_accuracy: 0.8881\n",
      "Epoch 2/15\n",
      "11396/11396 [==============================] - 9s 778us/step - loss: 0.3285 - accuracy: 0.8816 - val_loss: 0.3251 - val_accuracy: 0.8632\n",
      "Epoch 3/15\n",
      "11396/11396 [==============================] - 9s 829us/step - loss: 0.3064 - accuracy: 0.8858 - val_loss: 0.3024 - val_accuracy: 0.8870\n",
      "Epoch 4/15\n",
      "11396/11396 [==============================] - 10s 881us/step - loss: 0.3130 - accuracy: 0.8829 - val_loss: 0.2998 - val_accuracy: 0.8895\n",
      "Epoch 5/15\n",
      "11396/11396 [==============================] - 10s 892us/step - loss: 0.3044 - accuracy: 0.8881 - val_loss: 0.3279 - val_accuracy: 0.8842\n",
      "Epoch 6/15\n",
      "11396/11396 [==============================] - 10s 906us/step - loss: 0.3087 - accuracy: 0.8838 - val_loss: 0.2915 - val_accuracy: 0.8846\n",
      "Epoch 7/15\n",
      "11396/11396 [==============================] - 10s 920us/step - loss: 0.3000 - accuracy: 0.8907 - val_loss: 0.3017 - val_accuracy: 0.8860\n",
      "Epoch 8/15\n",
      "11396/11396 [==============================] - 11s 930us/step - loss: 0.3013 - accuracy: 0.8845 - val_loss: 0.3100 - val_accuracy: 0.8916\n",
      "Epoch 9/15\n",
      "11396/11396 [==============================] - 10s 913us/step - loss: 0.2264 - accuracy: 0.9204 - val_loss: 0.2208 - val_accuracy: 0.9225\n",
      "Epoch 10/15\n",
      "11396/11396 [==============================] - 10s 906us/step - loss: 0.2115 - accuracy: 0.9279 - val_loss: 0.2154 - val_accuracy: 0.9277\n",
      "Epoch 11/15\n",
      "11396/11396 [==============================] - 10s 912us/step - loss: 0.2046 - accuracy: 0.9290 - val_loss: 0.2134 - val_accuracy: 0.9295\n",
      "Epoch 12/15\n",
      "11396/11396 [==============================] - 10s 908us/step - loss: 0.2028 - accuracy: 0.9333 - val_loss: 0.2135 - val_accuracy: 0.9263\n",
      "Epoch 13/15\n",
      "11396/11396 [==============================] - 10s 909us/step - loss: 0.2019 - accuracy: 0.9310 - val_loss: 0.2157 - val_accuracy: 0.9249\n",
      "Epoch 14/15\n",
      "11396/11396 [==============================] - 10s 904us/step - loss: 0.1926 - accuracy: 0.9359 - val_loss: 0.2135 - val_accuracy: 0.9298\n",
      "Epoch 15/15\n",
      "11396/11396 [==============================] - 11s 922us/step - loss: 0.1900 - accuracy: 0.9388 - val_loss: 0.2126 - val_accuracy: 0.9305\n",
      "Training finished \n",
      "\n",
      "3562/3562 [==============================] - 1s 305us/step\n",
      "Evaluation on test data: loss = 0.204255 accuracy = 92.95% \n",
      "\n",
      "调用函数auc： 0.9711382706490972\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 32, 32)            38432     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 25, 64)            16448     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 25, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 18, 128)           65664     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 18, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2305      \n",
      "=================================================================\n",
      "Total params: 122,849\n",
      "Trainable params: 122,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "auc： 0.9711382706490972\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "0.9295\n",
      "0.8503\n",
      "0.9711\n",
      "0.9418\n",
      "0.9094\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout,GlobalAveragePooling1D\n",
    "from keras.layers import Conv2D,MaxPool2D,Dense,Flatten\n",
    "from sklearn.metrics import roc_curve, auc,roc_auc_score \n",
    "from keras.layers import TimeDistributed ,advanced_activations\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras as K\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "nb_filter=32\n",
    "filter_length=8\n",
    "def creat_model():\n",
    "    global xx\n",
    "    init = K.initializers.glorot_uniform()\n",
    "    #scheduler = keras.callbacks.ReduceLROnPlateau(simple_adam, 'max', factor=0.5, patience=3)\n",
    "    model = K.models.Sequential()\n",
    "\n",
    "\n",
    "    #model.add(Convolution1D(nb_filter=64, filter_length=1, input_shape=(500, 10)))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Flatten())\n",
    "    #model.add(Dropout(0.4))\n",
    "    # 定义卷积层\n",
    "\n",
    "    # 编译模型\n",
    "    model = keras.Sequential()\n",
    "    acctivation=keras.layers.advanced_activations.LeakyReLU()  \n",
    "\n",
    "    model.add(Convolution1D(nb_filter, filter_length, strides=1, padding='valid'))\n",
    "#    model.add(keras.layers.MaxPool1D(pool_size=2, strides=2, padding=\"valid\"))\n",
    "\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Convolution1D(2*nb_filter, filter_length, strides=1, padding='valid'))\n",
    "#    model.add(keras.layers.MaxPool1D(pool_size=2, strides=2, padding=\"valid\"))\n",
    "\n",
    "    model.add(Dropout(0.2))    \n",
    "    model.add(Convolution1D(4*nb_filter, filter_length, strides=1, padding='valid'))\n",
    "    model.add(keras.layers.MaxPool1D(pool_size=2, strides=2, padding=\"valid\"))\n",
    "#\n",
    "    model.add(Dropout(0.3))  \n",
    "#\n",
    "#   model.add(Convolution1D(4*nb_filter, filter_length, strides=1, padding='valid'))\n",
    "   # model.add(keras.layers.MaxPool1D(pool_size=2, strides=2, padding=\"valid\"))\n",
    "  #  model.add(Dropout(0.2))\n",
    "\n",
    " #   model.add(TimeDistributed(Dense(32)))\n",
    "#    model.add(keras.layers.LSTM(32, return_sequences=True))\n",
    "#    model.add(Bidirectional(LSTM(16, return_sequences=True)))\n",
    "\n",
    "#    model.add(CRF(32))\n",
    "    model.add(Flatten())\n",
    "\n",
    "  #  model.add(K.layers.Dense(units=100 ,kernel_initializer=init, activation='relu'))\n",
    "   # model.add(Dropout(0.2))\n",
    "#    model.add(K.layers.Dense(units=100 ,kernel_initializer=init, activation='relu'))\n",
    "   # model.add(Dropout(0.2))\n",
    "    model.add(K.layers.Dense(units=1, kernel_initializer=init, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(0.001),metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def train():\n",
    "    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n",
    "    for train, test in kfold.split(xx, xx_y):\n",
    "        train_x=xx[train]\n",
    "        train_y=xx_y[train]\n",
    "        test_x=xx[test]\n",
    "        test_y=xx_y[test]\n",
    "        train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "        print(train_x.shape,test_x.shape,val_x.shape)\n",
    "        model = creat_model()\n",
    "        max_epochs = 10\n",
    "        print(\"Starting training \")\n",
    "        h = model.fit(train_x, train_y, epochs=max_epochs, batch_size=128,shuffle=True, verbose=1,callbacks=[history],validation_data=(val_x, val_y))\n",
    "        print(\"Training finished \\n\")\n",
    "\n",
    "        eval = model.evaluate(test_x, test_y, verbose=1)\n",
    "        print(\"Evaluation on test data: loss = %0.6f accuracy = %0.2f%% \\n\" % (eval[0], eval[1] * 100) )\n",
    "        print(model.summary())\n",
    "\n",
    "def train2():\n",
    "    train_x, test_x, train_y, test_y = train_test_split(xx, xx_y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "    #train_x, val_x, train_y, val_y = train_test_split(xx, xx_y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "    print(train_x.shape,test_x.shape,val_x.shape)\n",
    "    model = creat_model()\n",
    "    max_epochs =15\n",
    "    \n",
    "    from keras.callbacks import ReduceLROnPlateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=2, mode='auto',factor=0.1)\n",
    "    print(reduce_lr)\n",
    "    print(\"Starting training \")\n",
    "    h = model.fit(train_x, train_y, epochs=max_epochs, batch_size=32,shuffle=True, verbose=1,callbacks=[reduce_lr,history],validation_data=(val_x, val_y))\n",
    "    print(\"Training finished \\n\")\n",
    "\n",
    "    scores = model.evaluate(test_x, test_y, verbose=1)\n",
    "    print(\"Evaluation on test data: loss = %0.6f accuracy = %0.2f%% \\n\" % (scores[0],scores[1] * 100) )\n",
    "    y_score = model.predict_proba(test_x)\n",
    "    print ('调用函数auc：', roc_auc_score(test_y, y_score))\n",
    "    print(model.summary())\n",
    "#    pd.DataFrame(test_y).to_csv('yy.csv',header=None,index=False)\n",
    "#    pd.DataFrame(y_score).to_csv('xx.csv',header=None,index=False)\n",
    "    fpr, tpr, thresholds = roc_curve(test_y.ravel(),y_score.ravel())\n",
    "    aucc = auc(fpr, tpr)\n",
    "    print ('auc：', aucc)\n",
    "#     plt.figure()\n",
    "\n",
    "#     plt.rcParams['figure.dpi'] = 350\n",
    "#     plt.plot(fpr, tpr, c = 'r', lw = 2, alpha = 0.7, label = 'AUC=%.3f' % aucc)\n",
    "#     plt.plot((0, 1), (0, 1), c = '#808080', lw = 1, ls = '--', alpha = 0.7)\n",
    "#     plt.xlim((-0.01, 1.02))\n",
    "#     plt.ylim((-0.01, 1.02))\n",
    "#     plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "#     plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "#     plt.xlabel('False Positive Rate', fontsize=13)\n",
    "#     plt.ylabel('True Positive Rate', fontsize=13)\n",
    "#     plt.grid(b=True, ls=':')\n",
    "#     plt.legend(loc='lower right', fancybox=True, framealpha=0.8, fontsize=12)\n",
    "#     plt.title('AUC', fontsize=17)\n",
    "#     plt.show()\n",
    "  \n",
    "#    Z_K = model.predict(test_x)\n",
    "#    pd.DataFrame(Z_K).to_csv(str(eval[1] * 100)+'zzz_d.csv',header=None,index=False)\n",
    "    y_predict = model.predict(test_x)\n",
    "    for i in range(len(y_predict)):\n",
    "        if(y_predict[i]<0.5):\n",
    "            y_predict[i]=0\n",
    "        else:\n",
    "            y_predict[i]=1\n",
    "    print(y_predict)\n",
    "    pd.DataFrame(y_predict).to_csv('y_predict.csv',header=None,index=False)\n",
    "\n",
    "    ACC=metrics.accuracy_score(test_y,y_predict)\n",
    "    MCC=metrics.matthews_corrcoef(test_y,y_predict)\n",
    "    ROC_AUC_area=metrics.roc_auc_score(test_y,y_score)\n",
    "    precision, recall, SN, SP, GM, TP, TN, FP, FN = performance(test_y, y_predict)\n",
    "\n",
    "    ACC=round(ACC,4)\n",
    "    MCC=round(MCC,4)\n",
    "    ROC_AUC_area=round(ROC_AUC_area,4)\n",
    "    SN=round(SN,4)\n",
    "    SP=round(SP,4)\n",
    "    print(ACC)\n",
    "    print(MCC)\n",
    "    print(ROC_AUC_area)\n",
    "    print(SN)\n",
    "    print(SP)\n",
    "if __name__ == '__main__':\n",
    " #   train()\n",
    "    train2()\n",
    "    import winsound \n",
    "\n",
    "    duration = 1000  # millisecond\n",
    "    freq = 440  # Hz\n",
    "    winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
